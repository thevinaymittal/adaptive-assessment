# Adaptive Language Assessment System of Tulkka

An intelligent English proficiency testing platform that adapts questions in real-time based on student performance, providing accurate CEFR level (A1-C2) placement in just 10 questions.

Imagine you're starting a language learning app. How do you quickly figure out if a student is a beginner (A1) or advanced (C1)? 

Traditional tests ask 50+ questions at random difficulty levels - boring and time-consuming. Our system is **smart**:

- Student gets a **medium-difficulty question** (B1 level)
- **Answers correctly?** â†’ Next question is **harder** (B2)
- **Answers incorrectly?** â†’ Next question is **easier** (A2)
- After just **10 adaptive questions**, we know their true level with **85%+ confidence**

## Key Features

### 1. **Adaptive Testing Algorithm**
- Starts at intermediate level (B1)
- Adjusts difficulty after each answer
- Tests multiple skills: grammar, vocabulary, reading, listening
- Reaches accurate level placement in 10 questions vs. 50+ in traditional tests

### 2. **CEFR Level Detection**
- **A1**: Beginner
- **A2**: Elementary
- **B1**: Intermediate
- **B2**: Upper Intermediate
- **C1**: Advanced
- **C2**: Proficient

### 3. **Question Validation System**
- Automatically detects if questions are too easy/hard
- Tracks which student levels struggle with each question
- Flags misclassified questions for expert review
- Generates calibration reports to maintain quality

### 4. **Bulk Import**
- Import 100s of questions via CSV
- Automatic validation of question format
- Detailed error reporting for failed imports
- Track import history and success rates

## Quick Start

### Prerequisites
```bash
- Python 3.8+
- MySQL 5.7+
- pip (Python package manager)
```

### Installation

1. **Clone the repository**
```bash
git clone adaptive-assessment.git
cd adaptive-assessment
```

2. **Install dependencies**
```bash
pip install fastapi uvicorn mysql-connector-python python-dotenv pydantic pandas numpy
```

3. **Setup database**
```bash
# Create database
mysql -u root -p
CREATE DATABASE tulkka_live;

# Run schema migrations (see SQL files in /db folder)
mysql -u root -p tulkka_live < db/assessment_tables.sql
mysql -u root -p tulkka_live < db/validation_tables.sql
```

4. **Configure environment**
```bash
# Create .env file
cp .env.example .env

# Edit .env with your database credentials
DB_HOST=3.xxx.xxx.xx3
DB_USER=axxxn
DB_PASSWORD=your_password
DB_NAME=tulkka_live
DB_PORT=3306
```

5. **Run the servers**
```bash
# Terminal 1: Assessment API (port 8000)
python api.py

# Terminal 2: Validation API (port 8001)
python validation_system.py
```

6. **Access API documentation**
- Assessment API: http://localhost:8000/docs
- Validation API: http://localhost:8001/docs

## How It Works

### For Students (Assessment Flow)

```
1. Student signs up
   â†“
2. System starts assessment session
   â†“
3. First question: B1 grammar (warm-up)
   â†“
4. Student answers
   â†“
5. System adapts:
   â€¢ Correct â†’ Harder question (B2)
   â€¢ Incorrect â†’ Easier question (A2)
   â†“
6. Repeat for 10 questions
   (rotating through grammar, vocabulary, reading, listening)
   â†“
7. Calculate final level
   â€¢ Find highest level with 60%+ accuracy
   â€¢ At least 2 questions at that level
   â†“
8. Display results:
   â€¢ Detected Level: B2
   â€¢ Confidence: 87%
   â€¢ Time taken: 4 minutes
```

### Example Student Journey

**Question 1 (B1):** "If I ___ rich, I would travel the world."
- Student selects: "were" âœ“ **Correct!**

**Question 2 (B2):** "By next year, I ___ here for five years."
- Student selects: "will have been working" âœ“ **Correct!**

**Question 3 (C1):** "Scarcely ___ when the meeting started."
- Student selects: "he had arrived" âœ— **Incorrect**

**Question 4 (B2):** "What does 'meticulous' mean?"
- Student selects: "Very careful" âœ“ **Correct!**

**... 6 more questions ...**

**Result:** Student is **B2 level** with **85% confidence**

### For Administrators (Quality Control)

```
1. Import questions via CSV
   â†“
2. Students take assessments
   â†“
3. System collects performance data
   â†“
4. Run calibration report (monthly)
   â†“
5. Review flagged questions:
   â€¢ Too easy? (>85% accuracy)
   â€¢ Too hard? (<40% accuracy)
   â€¢ Inconsistent performance?
   â†“
6. Reclassify misclassified questions
   â†“
7. Track improvement over time
```

## Real-World Example

### Before Calibration
```
Question #42 (Classified as B2):
"Choose: She ___ to school every day."
Options: go, goes, going, gone

Performance:
- A1 students: 85% correct TOO EASY!
- B2 students: 92% correct TOO EASY!

System flags: "Recommended level: A1"
```

### After Reclassification
```
Question #42 (Reclassified to A1):
Now correctly placed at beginner level âœ“
```

## Use Cases

### 1. **Language Schools**
- Place new students in appropriate classes
- Avoid beginner students in advanced classes (reduces churn)
- Track student progress with periodic re-testing

### 2. **Online Learning Platforms**
- Personalize learning paths based on accurate level
- Match students with appropriate teachers
- Demonstrate tangible progress to students

### 3. **Corporate Training**
- Assess employee language skills quickly
- Tailor training programs to actual needs
- Track ROI of training programs

### 4. **Educational Research**
- Study language acquisition patterns
- Analyze which question types predict proficiency
- Compare assessment methods

## Benefits Over Traditional Testing

| Feature | Traditional Test | Adaptive Assessment |
|---------|-----------------|-------------------|
| **Questions needed** | 50-100 | 10 |
| **Time required** | 30-60 minutes | 5-8 minutes |
| **Student experience** | Boring, repetitive | Engaging, challenging |
| **Accuracy** | 70-80% | 85-95% |
| **Drop-off rate** | 40-50% | 10-15% |
| **Cost per assessment** | High (manual review) | Low (automated) |

##Technical Stack

- **Backend**: FastAPI (Python)
- **Database**: MySQL 8.0
- **Adaptive Algorithm**: Item Response Theory (IRT) inspired
- **API Documentation**: OpenAPI/Swagger
- **Data Processing**: Pandas, NumPy
- **Frontend**: Any (React, Vue, Angular) - API-first design

## CSV Import Format

Create a CSV file with these columns:

```csv
question_text,question_type,difficulty_level,skill_focus,option_1,option_2,option_3,option_4,option_5,option_6,correct_answer,explanation
"Choose: She ___ every day.",multiple_choice,A1,grammar,go,goes,going,gone,,,goes,Third person singular
"Complete: If I ___ rich...",multiple_choice,B1,grammar,am,was,were,be,,,were,Second conditional
```

**Required columns:**
- `question_text` - The question
- `question_type` - multiple_choice, fill_blank, ordering, audio_response
- `difficulty_level` - A1, A2, B1, B2, C1, C2
- `skill_focus` - grammar, vocabulary, reading, listening
- `option_1` through `option_6` - Answer choices (min 2 required)
- `correct_answer` - Must match one of the options exactly
- `explanation` - Optional explanation for the answer

## Getting Your First Questions

### Option 1: Hire CEFR Experts
- Engage certified English teachers
- Request 20-30 questions per level per skill (480 total)
- Cost: $1,000-3,000 for quality question bank

### Option 2: Adapt Existing Content
- Use textbook exercises (check copyright!)
- Map to CEFR levels using official descriptors
- Have teachers review classification

### Option 3: Generate with AI (Review Required!)
- Use ChatGPT/Claude to generate questions
- **CRITICAL**: Have human experts review EVERY question
- AI often misclassifies difficulty levels

### Minimum Recommended:
- **5 questions per level per skill** = 120 questions minimum
- **20 questions per level per skill** = 480 questions ideal
- **More is better** for accurate adaptive testing

## Success Metrics

Track these KPIs to measure system success:

### Assessment Quality
- **Completion Rate**: % of students who finish (target: >85%)
- **Average Confidence Score**: How sure the system is (target: >80%)
- **Self-Report vs Detected Gap**: How many levels off students are (target: <1 level)

### Question Bank Health
- **Questions Needing Review**: % flagged questions (target: <20%)
- **Level Accuracy**: % well-calibrated per level (target: >70%)
- **Coverage Gaps**: Levels with <5 questions (target: 0 gaps)

### Business Impact
- **Student Churn**: Reduced by proper placement (target: -30%)
- **Teacher Satisfaction**: Better matched students (target: +40%)
- **Assessment Time**: Faster onboarding (target: <10 min)

## Common Pitfalls

### 1. **Insufficient Questions**
âŒ Problem: Only 50 total questions
âœ… Solution: Minimum 120 (5 per level/skill), ideally 480+

### 2. **Poor Level Classification**
âŒ Problem: Questions marked as B2 but actually A2
âœ… Solution: Run calibration after 50+ student responses, reclassify

### 3. **Skill Imbalance**
âŒ Problem: 80 grammar questions, 10 listening questions
âœ… Solution: Equal distribution across all 4 skills

### 4. **Ignoring Calibration Data**
âŒ Problem: Never reviewing flagged questions
âœ… Solution: Monthly calibration reports, quarterly expert review

### 5. **Not Testing the Test**
âŒ Problem: Launch without pilot testing
âœ… Solution: Pilot with 50-100 students, validate against manual assessments

## ðŸ› ï¸ Maintenance

### Monthly Tasks
- Run calibration report
- Review top 10 flagged questions
- Reclassify obvious misclassifications

### Quarterly Tasks
- Expert review of all flagged questions
- Add new questions to fill gaps
- Analyze student feedback

### Annually
- Comprehensive question bank audit
- Update questions to reflect current language usage
- Benchmark against standardized tests (IELTS, TOEFL)

## Further Reading

- [CEFR Official Framework](https://www.coe.int/en/web/common-european-framework-reference-languages)
- [Item Response Theory Basics](https://en.wikipedia.org/wiki/Item_response_theory)
- [Adaptive Testing Best Practices](https://www.cambridge.org/elt/blog/2019/07/22/adaptive-testing/)

---
